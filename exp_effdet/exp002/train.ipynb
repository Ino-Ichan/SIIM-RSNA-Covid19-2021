{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12bbb380",
   "metadata": {},
   "source": [
    "blog\n",
    "\n",
    "https://medium.com/data-science-at-microsoft/training-efficientdet-on-custom-data-with-pytorch-lightning-using-an-efficientnetv2-backbone-1cdf3bd7921f\n",
    "\n",
    "code\n",
    "\n",
    "https://gist.github.com/Chris-hughes10/73628b1d8d6fc7d359b3dcbbbb8869d7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "606d54ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "\n",
    "from pathlib import Path\n",
    "import PIL\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from effdet.config.model_config import efficientdet_model_param_dict\n",
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n",
    "from effdet.efficientdet import HeadNet\n",
    "from effdet.config.model_config import efficientdet_model_param_dict\n",
    "\n",
    "import timm\n",
    "\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning.core.decorators import auto_move_data\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "from numbers import Number\n",
    "from typing import List\n",
    "from functools import singledispatch\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from fastcore.dispatch import typedispatch\n",
    "from fastcore.basics import patch\n",
    "\n",
    "from ensemble_boxes import ensemble_boxes_wbf\n",
    "from objdetecteval.metrics.coco_metrics import get_coco_stats\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Plot function\n",
    "# ====================================================\n",
    "def get_rectangle_edges_from_pascal_bbox(bbox):\n",
    "    xmin_top_left, ymin_top_left, xmax_bottom_right, ymax_bottom_right = bbox\n",
    "\n",
    "    bottom_left = (xmin_top_left, ymax_bottom_right)\n",
    "    width = xmax_bottom_right - xmin_top_left\n",
    "    height = ymin_top_left - ymax_bottom_right\n",
    "\n",
    "    return bottom_left, width, height\n",
    "\n",
    "def draw_pascal_voc_bboxes(\n",
    "    plot_ax,\n",
    "    bboxes,\n",
    "    get_rectangle_corners_fn=get_rectangle_edges_from_pascal_bbox,\n",
    "):\n",
    "    for bbox in bboxes:\n",
    "        bottom_left, width, height = get_rectangle_corners_fn(bbox)\n",
    "\n",
    "        rect_1 = patches.Rectangle(\n",
    "            bottom_left,\n",
    "            width,\n",
    "            height,\n",
    "            linewidth=4,\n",
    "            edgecolor=\"black\",\n",
    "            fill=False,\n",
    "        )\n",
    "        rect_2 = patches.Rectangle(\n",
    "            bottom_left,\n",
    "            width,\n",
    "            height,\n",
    "            linewidth=2,\n",
    "            edgecolor=\"white\",\n",
    "            fill=False,\n",
    "        )\n",
    "\n",
    "        # Add the patch to the Axes\n",
    "        plot_ax.add_patch(rect_1)\n",
    "        plot_ax.add_patch(rect_2)\n",
    "\n",
    "def show_image(\n",
    "    image, bboxes=None, draw_bboxes_fn=draw_pascal_voc_bboxes, figsize=(10, 10)\n",
    "):\n",
    "    fig, ax = plt.subplots(1, figsize=figsize)\n",
    "    ax.imshow(image)\n",
    "\n",
    "    if bboxes is not None:\n",
    "        draw_bboxes_fn(ax, bboxes)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Detection function\n",
    "# ====================================================\n",
    "\n",
    "def run_wbf(predictions, image_size=512, iou_thr=0.44, skip_box_thr=0.43, weights=None):\n",
    "    bboxes = []\n",
    "    confidences = []\n",
    "    class_labels = []\n",
    "\n",
    "    for prediction in predictions:\n",
    "        boxes = [(prediction[\"boxes\"] / image_size).tolist()]\n",
    "        scores = [prediction[\"scores\"].tolist()]\n",
    "        labels = [prediction[\"classes\"].tolist()]\n",
    "\n",
    "        boxes, scores, labels = ensemble_boxes_wbf.weighted_boxes_fusion(\n",
    "            boxes,\n",
    "            scores,\n",
    "            labels,\n",
    "            weights=weights,\n",
    "            iou_thr=iou_thr,\n",
    "            skip_box_thr=skip_box_thr,\n",
    "        )\n",
    "        boxes = boxes * (image_size - 1)\n",
    "        bboxes.append(boxes.tolist())\n",
    "        confidences.append(scores.tolist())\n",
    "        class_labels.append(labels.tolist())\n",
    "\n",
    "    return bboxes, confidences, class_labels\n",
    "\n",
    "# ====================================================\n",
    "# Dataset, Dataloader\n",
    "# ====================================================\n",
    "\n",
    "class DatasetAdaptor:\n",
    "    def __init__(self, images_dir_path, annotations_dataframe):\n",
    "        self.images_dir_path = Path(images_dir_path)\n",
    "        self.annotations_df = annotations_dataframe\n",
    "        self.images = self.annotations_df.image.unique().tolist()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    def get_image_and_labels_by_idx(self, index):\n",
    "        image_name = self.images[index]\n",
    "        image = PIL.Image.open(self.images_dir_path / image_name).convert(\"RGB\")\n",
    "        pascal_bboxes = self.annotations_df[self.annotations_df.image == image_name][\n",
    "            [\"xmin\", \"ymin\", \"xmax\", \"ymax\"]\n",
    "        ].values\n",
    "        class_labels = np.ones(len(pascal_bboxes))\n",
    "\n",
    "        return image, pascal_bboxes, class_labels, index\n",
    "    \n",
    "    def show_image(self, index):\n",
    "        image, bboxes, class_labels, image_id = self.get_image_and_labels_by_idx(index)\n",
    "        print(f\"image_id: {image_id}\")\n",
    "        show_image(image, bboxes.tolist())\n",
    "        print(class_labels)\n",
    "\n",
    "\n",
    "class EfficientDetDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, dataset_adaptor, transforms\n",
    "    ):\n",
    "        self.ds = dataset_adaptor\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        (\n",
    "            image,\n",
    "            pascal_bboxes,\n",
    "            class_labels,\n",
    "            image_id,\n",
    "        ) = self.ds.get_image_and_labels_by_idx(index)\n",
    "\n",
    "        sample = {\n",
    "            \"image\": np.array(image, dtype=np.float32),\n",
    "            \"bboxes\": pascal_bboxes,\n",
    "            \"labels\": class_labels,\n",
    "        }\n",
    "\n",
    "        sample = self.transforms(**sample)\n",
    "        sample[\"bboxes\"] = np.array(sample[\"bboxes\"])\n",
    "        image = sample[\"image\"]\n",
    "        pascal_bboxes = sample[\"bboxes\"]\n",
    "        labels = sample[\"labels\"]\n",
    "\n",
    "        _, new_h, new_w = image.shape\n",
    "        sample[\"bboxes\"][:, [0, 1, 2, 3]] = sample[\"bboxes\"][\n",
    "            :, [1, 0, 3, 2]\n",
    "        ]  # convert to yxyx\n",
    "\n",
    "        target = {\n",
    "            \"bboxes\": torch.as_tensor(pascal_bboxes, dtype=torch.float32),\n",
    "            \"labels\": torch.as_tensor(labels),\n",
    "            \"image_id\": torch.tensor([image_id]),\n",
    "            \"img_size\": (new_h, new_w),\n",
    "            \"img_scale\": torch.tensor([1.0]),\n",
    "        }\n",
    "\n",
    "        return image, target, image_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "\n",
    "class EfficientDetDataModule(LightningDataModule):\n",
    "    \n",
    "    def __init__(self,\n",
    "                train_dataset_adaptor,\n",
    "                validation_dataset_adaptor,\n",
    "                train_transforms,\n",
    "                valid_transforms,\n",
    "                num_workers=4,\n",
    "                batch_size=8):\n",
    "        \n",
    "        self.train_ds = train_dataset_adaptor\n",
    "        self.valid_ds = validation_dataset_adaptor\n",
    "        self.train_tfms = train_transforms\n",
    "        self.valid_tfms = valid_transforms\n",
    "        self.num_workers = num_workers\n",
    "        self.batch_size = batch_size\n",
    "        super().__init__()\n",
    "\n",
    "    def train_dataset(self) -> EfficientDetDataset:\n",
    "        return EfficientDetDataset(\n",
    "            dataset_adaptor=self.train_ds, transforms=self.train_tfms\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        train_dataset = self.train_dataset()\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataset(self) -> EfficientDetDataset:\n",
    "        return EfficientDetDataset(\n",
    "            dataset_adaptor=self.valid_ds, transforms=self.valid_tfms\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        valid_dataset = self.val_dataset()\n",
    "        valid_loader = torch.utils.data.DataLoader(\n",
    "            valid_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "        return valid_loader\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        images, targets, image_ids = tuple(zip(*batch))\n",
    "        images = torch.stack(images)\n",
    "        images = images.float()\n",
    "\n",
    "        boxes = [target[\"bboxes\"].float() for target in targets]\n",
    "        labels = [target[\"labels\"].float() for target in targets]\n",
    "        img_size = torch.tensor([target[\"img_size\"] for target in targets]).float()\n",
    "        img_scale = torch.tensor([target[\"img_scale\"] for target in targets]).float()\n",
    "\n",
    "        annotations = {\n",
    "            \"bbox\": boxes,\n",
    "            \"cls\": labels,\n",
    "            \"img_size\": img_size,\n",
    "            \"img_scale\": img_scale,\n",
    "        }\n",
    "\n",
    "        return images, annotations, targets, image_ids\n",
    "    \n",
    "\n",
    "# ====================================================\n",
    "# Transform\n",
    "# ====================================================\n",
    "\n",
    "def get_train_transforms(train_image_size=0):\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(train_image_size, train_image_size),\n",
    "#             A.HorizontalFlip(p=0.5),\n",
    "#             A.RandomBrightness(limit=0.2, p=0.75),\n",
    "#             A.RandomContrast(limit=0.2, p=0.75),\n",
    "\n",
    "            # albumentations.OneOf([\n",
    "            #     albumentations.OpticalDistortion(distort_limit=1.),\n",
    "            #     # albumentations.GridDistortion(num_steps=5, distort_limit=1.),\n",
    "            # ], p=0.75),\n",
    "\n",
    "#             A.HueSaturationValue(hue_shift_limit=40, sat_shift_limit=40, val_shift_limit=0, p=0.75),\n",
    "            # albumentations.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.3, rotate_limit=30, border_mode=0, p=0.75),\n",
    "#             A.ShiftScaleRotate(shift_limit=0., scale_limit=0.2, rotate_limit=15, border_mode=0, p=0.75),\n",
    "            # CutoutV2(max_h_size=int(image_size * 0.2), max_w_size=int(image_size * 0.2), num_holes=2, p=0.75),\n",
    "#             A.Cutout(max_h_size=int(train_image_size * 0.1), max_w_size=int(train_image_size * 0.1), num_holes=2, p=0.5),\n",
    "            ToTensorV2(p=1)\n",
    "        ], \n",
    "        p=1.0, \n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0, \n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def get_valid_transforms(val_image_size=0):\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=val_image_size, width=val_image_size, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], \n",
    "        p=1.0, \n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0, \n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "\n",
    "def create_model(num_classes=1, image_size=512, architecture=\"tf_efficientnetv2_l\"):\n",
    "    if architecture not in efficientdet_model_param_dict.keys():\n",
    "        efficientdet_model_param_dict[architecture] = dict(\n",
    "            name=architecture,\n",
    "            backbone_name=architecture,\n",
    "            backbone_args=dict(drop_path_rate=0.2),\n",
    "            num_classes=num_classes,\n",
    "            url='', )\n",
    "    \n",
    "    config = get_efficientdet_config(architecture)\n",
    "    config.update({'num_classes': num_classes})\n",
    "    config.update({'image_size': (image_size, image_size)})\n",
    "    \n",
    "    print(config)\n",
    "\n",
    "    net = EfficientDet(config, pretrained_backbone=True)\n",
    "    net.class_net = HeadNet(\n",
    "        config,\n",
    "        num_outputs=config.num_classes,\n",
    "    )\n",
    "    return DetBenchTrain(net, config)\n",
    "\n",
    "\n",
    "class EfficientDetModel(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=1,\n",
    "        img_size=512,\n",
    "        prediction_confidence_threshold=0.2,\n",
    "        learning_rate=0.0002,\n",
    "        wbf_iou_threshold=0.44,\n",
    "        inference_transforms=get_valid_transforms(),\n",
    "        model_architecture='tf_efficientnetv2_l',\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.model = create_model(\n",
    "            num_classes, img_size, architecture=model_architecture\n",
    "        )\n",
    "        self.prediction_confidence_threshold = prediction_confidence_threshold\n",
    "        self.lr = learning_rate\n",
    "        self.wbf_iou_threshold = wbf_iou_threshold\n",
    "        self.inference_tfms = inference_transforms\n",
    "\n",
    "\n",
    "    @auto_move_data\n",
    "    def forward(self, images, targets):\n",
    "        return self.model(images, targets)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, annotations, _, image_ids = batch\n",
    "\n",
    "        losses = self.model(images, annotations)\n",
    "\n",
    "        logging_losses = {\n",
    "            \"class_loss\": losses[\"class_loss\"].detach(),\n",
    "            \"box_loss\": losses[\"box_loss\"].detach(),\n",
    "        }\n",
    "\n",
    "        self.log(\"train_loss\", losses[\"loss\"], on_step=True, on_epoch=True, prog_bar=True,\n",
    "                 logger=True)\n",
    "        self.log(\n",
    "            \"train_class_loss\", losses[\"class_loss\"], on_step=True, on_epoch=True, prog_bar=True,\n",
    "            logger=True\n",
    "        )\n",
    "        self.log(\"train_box_loss\", losses[\"box_loss\"], on_step=True, on_epoch=True, prog_bar=True,\n",
    "                 logger=True)\n",
    "\n",
    "        return losses['loss']\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, annotations, targets, image_ids = batch\n",
    "        outputs = self.model(images, annotations)\n",
    "\n",
    "        detections = outputs[\"detections\"]\n",
    "\n",
    "        batch_predictions = {\n",
    "            \"predictions\": detections,\n",
    "            \"targets\": targets,\n",
    "            \"image_ids\": image_ids,\n",
    "        }\n",
    "\n",
    "        logging_losses = {\n",
    "            \"class_loss\": outputs[\"class_loss\"].detach(),\n",
    "            \"box_loss\": outputs[\"box_loss\"].detach(),\n",
    "        }\n",
    "\n",
    "        self.log(\"valid_loss\", outputs[\"loss\"], on_step=True, on_epoch=True, prog_bar=True,\n",
    "                 logger=True, sync_dist=True)\n",
    "        self.log(\n",
    "            \"valid_class_loss\", logging_losses[\"class_loss\"], on_step=True, on_epoch=True,\n",
    "            prog_bar=True, logger=True, sync_dist=True\n",
    "        )\n",
    "        self.log(\"valid_box_loss\", logging_losses[\"box_loss\"], on_step=True, on_epoch=True,\n",
    "                 prog_bar=True, logger=True, sync_dist=True)\n",
    "\n",
    "        return {'loss': outputs[\"loss\"], 'batch_predictions': batch_predictions}\n",
    "    \n",
    "    \n",
    "    @typedispatch\n",
    "    def predict(self, images: List):\n",
    "        \"\"\"\n",
    "        For making predictions from images\n",
    "        Args:\n",
    "            images: a list of PIL images\n",
    "\n",
    "        Returns: a tuple of lists containing bboxes, predicted_class_labels, predicted_class_confidences\n",
    "\n",
    "        \"\"\"\n",
    "        image_sizes = [(image.size[1], image.size[0]) for image in images]\n",
    "        images_tensor = torch.stack(\n",
    "            [\n",
    "                self.inference_tfms(\n",
    "                    image=np.array(image, dtype=np.float32),\n",
    "                    labels=np.ones(1),\n",
    "                    bboxes=np.array([[0, 0, 1, 1]]),\n",
    "                )[\"image\"]\n",
    "                for image in images\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return self._run_inference(images_tensor, image_sizes)\n",
    "\n",
    "    @typedispatch\n",
    "    def predict(self, images_tensor: torch.Tensor):\n",
    "        \"\"\"\n",
    "        For making predictions from tensors returned from the model's dataloader\n",
    "        Args:\n",
    "            images_tensor: the images tensor returned from the dataloader\n",
    "\n",
    "        Returns: a tuple of lists containing bboxes, predicted_class_labels, predicted_class_confidences\n",
    "\n",
    "        \"\"\"\n",
    "        if images_tensor.ndim == 3:\n",
    "            images_tensor = images_tensor.unsqueeze(0)\n",
    "        if (\n",
    "            images_tensor.shape[-1] != self.img_size\n",
    "            or images_tensor.shape[-2] != self.img_size\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                f\"Input tensors must be of shape (N, 3, {self.img_size}, {self.img_size})\"\n",
    "            )\n",
    "\n",
    "        num_images = images_tensor.shape[0]\n",
    "        image_sizes = [(self.img_size, self.img_size)] * num_images\n",
    "\n",
    "        return self._run_inference(images_tensor, image_sizes)\n",
    "\n",
    "    def _run_inference(self, images_tensor, image_sizes):\n",
    "        dummy_targets = self._create_dummy_inference_targets(\n",
    "            num_images=images_tensor.shape[0]\n",
    "        )\n",
    "\n",
    "        detections = self.model(images_tensor.to(self.device), dummy_targets)[\n",
    "            \"detections\"\n",
    "        ]\n",
    "        (\n",
    "            predicted_bboxes,\n",
    "            predicted_class_confidences,\n",
    "            predicted_class_labels,\n",
    "        ) = self.post_process_detections(detections)\n",
    "\n",
    "        scaled_bboxes = self.__rescale_bboxes(\n",
    "            predicted_bboxes=predicted_bboxes, image_sizes=image_sizes\n",
    "        )\n",
    "\n",
    "        return scaled_bboxes, predicted_class_labels, predicted_class_confidences\n",
    "    \n",
    "    def _create_dummy_inference_targets(self, num_images):\n",
    "        dummy_targets = {\n",
    "            \"bbox\": [\n",
    "                torch.tensor([[0.0, 0.0, 0.0, 0.0]], device=self.device)\n",
    "                for i in range(num_images)\n",
    "            ],\n",
    "            \"cls\": [torch.tensor([1.0], device=self.device) for i in range(num_images)],\n",
    "            \"img_size\": torch.tensor(\n",
    "                [(self.img_size, self.img_size)] * num_images, device=self.device\n",
    "            ).float(),\n",
    "            \"img_scale\": torch.ones(num_images, device=self.device).float(),\n",
    "        }\n",
    "\n",
    "        return dummy_targets\n",
    "    \n",
    "    def post_process_detections(self, detections):\n",
    "        predictions = []\n",
    "        for i in range(detections.shape[0]):\n",
    "            predictions.append(\n",
    "                self._postprocess_single_prediction_detections(detections[i])\n",
    "            )\n",
    "\n",
    "        predicted_bboxes, predicted_class_confidences, predicted_class_labels = run_wbf(\n",
    "            predictions, image_size=self.img_size, iou_thr=self.wbf_iou_threshold\n",
    "        )\n",
    "\n",
    "        return predicted_bboxes, predicted_class_confidences, predicted_class_labels\n",
    "\n",
    "    def _postprocess_single_prediction_detections(self, detections):\n",
    "        boxes = detections.detach().cpu().numpy()[:, :4]\n",
    "        scores = detections.detach().cpu().numpy()[:, 4]\n",
    "        classes = detections.detach().cpu().numpy()[:, 5]\n",
    "        indexes = np.where(scores > self.prediction_confidence_threshold)[0]\n",
    "        boxes = boxes[indexes]\n",
    "\n",
    "        return {\"boxes\": boxes, \"scores\": scores[indexes], \"classes\": classes[indexes]}\n",
    "\n",
    "    def __rescale_bboxes(self, predicted_bboxes, image_sizes):\n",
    "        scaled_bboxes = []\n",
    "        for bboxes, img_dims in zip(predicted_bboxes, image_sizes):\n",
    "            im_h, im_w = img_dims\n",
    "\n",
    "            if len(bboxes) > 0:\n",
    "                scaled_bboxes.append(\n",
    "                    (\n",
    "                        np.array(bboxes)\n",
    "                        * [\n",
    "                            im_w / self.img_size,\n",
    "                            im_h / self.img_size,\n",
    "                            im_w / self.img_size,\n",
    "                            im_h / self.img_size,\n",
    "                        ]\n",
    "                    ).tolist()\n",
    "                )\n",
    "            else:\n",
    "                scaled_bboxes.append(bboxes)\n",
    "\n",
    "        return scaled_bboxes\n",
    "\n",
    "\n",
    "@patch\n",
    "def aggregate_prediction_outputs(self: EfficientDetModel, outputs):\n",
    "\n",
    "    detections = torch.cat(\n",
    "        [output[\"batch_predictions\"][\"predictions\"] for output in outputs]\n",
    "    )\n",
    "\n",
    "    image_ids = []\n",
    "    targets = []\n",
    "    for output in outputs:\n",
    "        batch_predictions = output[\"batch_predictions\"]\n",
    "        image_ids.extend(batch_predictions[\"image_ids\"])\n",
    "        targets.extend(batch_predictions[\"targets\"])\n",
    "\n",
    "    (\n",
    "        predicted_bboxes,\n",
    "        predicted_class_confidences,\n",
    "        predicted_class_labels,\n",
    "    ) = self.post_process_detections(detections)\n",
    "\n",
    "    return (\n",
    "        predicted_class_labels,\n",
    "        image_ids,\n",
    "        predicted_bboxes,\n",
    "        predicted_class_confidences,\n",
    "        targets,\n",
    "    )\n",
    "\n",
    "@patch\n",
    "def validation_epoch_end(self: EfficientDetModel, outputs):\n",
    "    \"\"\"Compute and log training loss and accuracy at the epoch level.\"\"\"\n",
    "\n",
    "    validation_loss_mean = torch.stack(\n",
    "        [output[\"loss\"] for output in outputs]\n",
    "    ).mean()\n",
    "\n",
    "    (\n",
    "        predicted_class_labels,\n",
    "        image_ids,\n",
    "        predicted_bboxes,\n",
    "        predicted_class_confidences,\n",
    "        targets,\n",
    "    ) = self.aggregate_prediction_outputs(outputs)\n",
    "\n",
    "    truth_image_ids = [target[\"image_id\"].detach().item() for target in targets]\n",
    "    truth_boxes = [\n",
    "        target[\"bboxes\"].detach()[:, [1, 0, 3, 2]].tolist() for target in targets\n",
    "    ] # convert to xyxy for evaluation\n",
    "    truth_labels = [target[\"labels\"].detach().tolist() for target in targets]\n",
    "\n",
    "    stats = get_coco_stats(\n",
    "        prediction_image_ids=image_ids,\n",
    "        predicted_class_confidences=predicted_class_confidences,\n",
    "        predicted_bboxes=predicted_bboxes,\n",
    "        predicted_class_labels=predicted_class_labels,\n",
    "        target_image_ids=truth_image_ids,\n",
    "        target_bboxes=truth_boxes,\n",
    "        target_class_labels=truth_labels,\n",
    "    )['All']\n",
    "    self.log(\"val_mAP50\", stats[\"AP_all_IOU_0_50\"], on_step=False, on_epoch=True,\n",
    "                 prog_bar=False, logger=True, sync_dist=True)\n",
    "\n",
    "    return {\"val_loss\": validation_loss_mean, \"metrics\": stats}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cceae60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d3e11c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad57acef",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcce45c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = \"/workspace/data/df_train_study_level_npy640_3_w_bbox.csv\"\n",
    "data_path = \"/workspace/data/train_640_2/\"\n",
    "\n",
    "# model_architecture = 'tf_efficientnetv2_m'\n",
    "model_architecture = 'tf_efficientdet_d3'\n",
    "exp = \"exp002\"\n",
    "\n",
    "\n",
    "train_image_size = 512\n",
    "val_image_size = 512\n",
    "cv = 0\n",
    "\n",
    "num_workers=4\n",
    "batch_size=8\n",
    "\n",
    "max_epochs = 25\n",
    "prediction_confidence_threshold=0.0001\n",
    "learning_rate=1e-4\n",
    "wbf_iou_threshold=0.5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "exp_name = f\"{exp}_{model_architecture}_cv{cv}\"\n",
    "save_path = f\"/workspace/output/effdet_{exp}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac2886a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "287110f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "      <th>study_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>have_box</th>\n",
       "      <th>dicom_path</th>\n",
       "      <th>Negative for Pneumonia</th>\n",
       "      <th>...</th>\n",
       "      <th>Atypical Appearance</th>\n",
       "      <th>cv</th>\n",
       "      <th>npy_path</th>\n",
       "      <th>is_none</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>image</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04f41a8958f7_image</td>\n",
       "      <td>688.06282</td>\n",
       "      <td>966.82563</td>\n",
       "      <td>518.48212</td>\n",
       "      <td>1130.17438</td>\n",
       "      <td>6e4a0581cefe</td>\n",
       "      <td>04f41a8958f7</td>\n",
       "      <td>1</td>\n",
       "      <td>/workspace/data/train/6e4a0581cefe/018ed20fa9c...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/workspace/data/train_640_2/04f41a8958f7.png</td>\n",
       "      <td>0</td>\n",
       "      <td>688.06282</td>\n",
       "      <td>966.82563</td>\n",
       "      <td>1206.54494</td>\n",
       "      <td>2097.00001</td>\n",
       "      <td>04f41a8958f7.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04f41a8958f7_image</td>\n",
       "      <td>2482.36026</td>\n",
       "      <td>1636.77436</td>\n",
       "      <td>652.47168</td>\n",
       "      <td>652.47168</td>\n",
       "      <td>6e4a0581cefe</td>\n",
       "      <td>04f41a8958f7</td>\n",
       "      <td>1</td>\n",
       "      <td>/workspace/data/train/6e4a0581cefe/018ed20fa9c...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/workspace/data/train_640_2/04f41a8958f7.png</td>\n",
       "      <td>0</td>\n",
       "      <td>2482.36026</td>\n",
       "      <td>1636.77436</td>\n",
       "      <td>3134.83194</td>\n",
       "      <td>2289.24604</td>\n",
       "      <td>04f41a8958f7.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>04f41a8958f7_image</td>\n",
       "      <td>1235.67308</td>\n",
       "      <td>1628.03597</td>\n",
       "      <td>509.74353</td>\n",
       "      <td>559.26147</td>\n",
       "      <td>6e4a0581cefe</td>\n",
       "      <td>04f41a8958f7</td>\n",
       "      <td>1</td>\n",
       "      <td>/workspace/data/train/6e4a0581cefe/018ed20fa9c...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/workspace/data/train_640_2/04f41a8958f7.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1235.67308</td>\n",
       "      <td>1628.03597</td>\n",
       "      <td>1745.41661</td>\n",
       "      <td>2187.29744</td>\n",
       "      <td>04f41a8958f7.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0572ef0d0c1a_image</td>\n",
       "      <td>1818.65264</td>\n",
       "      <td>233.50598</td>\n",
       "      <td>613.04395</td>\n",
       "      <td>839.53784</td>\n",
       "      <td>adbfed2da701</td>\n",
       "      <td>0572ef0d0c1a</td>\n",
       "      <td>1</td>\n",
       "      <td>/workspace/data/train/adbfed2da701/e2fa197720c...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>/workspace/data/train_640_2/0572ef0d0c1a.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1818.65264</td>\n",
       "      <td>233.50598</td>\n",
       "      <td>2431.69659</td>\n",
       "      <td>1073.04382</td>\n",
       "      <td>0572ef0d0c1a.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0572ef0d0c1a_image</td>\n",
       "      <td>598.60492</td>\n",
       "      <td>61.37052</td>\n",
       "      <td>688.54175</td>\n",
       "      <td>881.81674</td>\n",
       "      <td>adbfed2da701</td>\n",
       "      <td>0572ef0d0c1a</td>\n",
       "      <td>1</td>\n",
       "      <td>/workspace/data/train/adbfed2da701/e2fa197720c...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>/workspace/data/train_640_2/0572ef0d0c1a.png</td>\n",
       "      <td>0</td>\n",
       "      <td>598.60492</td>\n",
       "      <td>61.37052</td>\n",
       "      <td>1287.14667</td>\n",
       "      <td>943.18726</td>\n",
       "      <td>0572ef0d0c1a.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id           x           y          w           h  \\\n",
       "0  04f41a8958f7_image   688.06282   966.82563  518.48212  1130.17438   \n",
       "1  04f41a8958f7_image  2482.36026  1636.77436  652.47168   652.47168   \n",
       "2  04f41a8958f7_image  1235.67308  1628.03597  509.74353   559.26147   \n",
       "3  0572ef0d0c1a_image  1818.65264   233.50598  613.04395   839.53784   \n",
       "4  0572ef0d0c1a_image   598.60492    61.37052  688.54175   881.81674   \n",
       "\n",
       "       study_id      image_id  have_box  \\\n",
       "0  6e4a0581cefe  04f41a8958f7         1   \n",
       "1  6e4a0581cefe  04f41a8958f7         1   \n",
       "2  6e4a0581cefe  04f41a8958f7         1   \n",
       "3  adbfed2da701  0572ef0d0c1a         1   \n",
       "4  adbfed2da701  0572ef0d0c1a         1   \n",
       "\n",
       "                                          dicom_path  Negative for Pneumonia  \\\n",
       "0  /workspace/data/train/6e4a0581cefe/018ed20fa9c...                       0   \n",
       "1  /workspace/data/train/6e4a0581cefe/018ed20fa9c...                       0   \n",
       "2  /workspace/data/train/6e4a0581cefe/018ed20fa9c...                       0   \n",
       "3  /workspace/data/train/adbfed2da701/e2fa197720c...                       0   \n",
       "4  /workspace/data/train/adbfed2da701/e2fa197720c...                       0   \n",
       "\n",
       "   ...  Atypical Appearance  cv                                      npy_path  \\\n",
       "0  ...                    0   0  /workspace/data/train_640_2/04f41a8958f7.png   \n",
       "1  ...                    0   0  /workspace/data/train_640_2/04f41a8958f7.png   \n",
       "2  ...                    0   0  /workspace/data/train_640_2/04f41a8958f7.png   \n",
       "3  ...                    0   3  /workspace/data/train_640_2/0572ef0d0c1a.png   \n",
       "4  ...                    0   3  /workspace/data/train_640_2/0572ef0d0c1a.png   \n",
       "\n",
       "   is_none        xmin        ymin        xmax        ymax             image  \\\n",
       "0        0   688.06282   966.82563  1206.54494  2097.00001  04f41a8958f7.png   \n",
       "1        0  2482.36026  1636.77436  3134.83194  2289.24604  04f41a8958f7.png   \n",
       "2        0  1235.67308  1628.03597  1745.41661  2187.29744  04f41a8958f7.png   \n",
       "3        0  1818.65264   233.50598  2431.69659  1073.04382  0572ef0d0c1a.png   \n",
       "4        0   598.60492    61.37052  1287.14667   943.18726  0572ef0d0c1a.png   \n",
       "\n",
       "   labels  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(df_path)\n",
    "\n",
    "# only have_bboxes\n",
    "df = df[df.have_box == 1].reset_index(drop=True)\n",
    "\n",
    "# column name change\n",
    "df[\"xmin\"] = [max(0, i) for i in df[\"x\"]]\n",
    "df[\"ymin\"] = [max(0, i) for i in df[\"y\"]]\n",
    "df[\"xmax\"] = df[\"x\"] + df[\"w\"]\n",
    "df[\"ymax\"] = df[\"y\"] + df[\"h\"]\n",
    "\n",
    "# # fillna\n",
    "# df[\"xmin\"] = df[\"xmin\"].fillna(0)\n",
    "# df[\"ymin\"] = df[\"ymin\"].fillna(0)\n",
    "# df[\"xmax\"] = df[\"xmax\"].fillna(1)\n",
    "# df[\"ymax\"] = df[\"ymax\"].fillna(1)\n",
    "\n",
    "# get image\n",
    "df[\"image\"] = [d.split(\"/\")[-1] for d in df.npy_path]\n",
    "df[\"labels\"] = df[\"have_box\"]\n",
    "\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4075be5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1e44ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data num: (857, 21)\n",
      "val data num: (857, 21)\n"
     ]
    }
   ],
   "source": [
    "# train_df = df[df.cv != cv].reset_index(drop=True)\n",
    "train_df = df[df.cv == cv].reset_index(drop=True)\n",
    "train_ds = DatasetAdaptor(data_path, train_df)\n",
    "\n",
    "val_df = df[df.cv == cv].reset_index(drop=True)\n",
    "val_ds = DatasetAdaptor(data_path, val_df)\n",
    "\n",
    "print(f\"train data num: {train_df.groupby('image').first().shape}\")\n",
    "print(f\"val data num: {val_df.groupby('image').first().shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62c7a31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'tf_efficientdet_d3', 'backbone_name': 'tf_efficientnet_b3', 'backbone_args': {'drop_path_rate': 0.2}, 'backbone_indices': None, 'image_size': [512, 512], 'num_classes': 1, 'min_level': 3, 'max_level': 7, 'num_levels': 5, 'num_scales': 3, 'aspect_ratios': [[1.0, 1.0], [1.4, 0.7], [0.7, 1.4]], 'anchor_scale': 4.0, 'pad_type': 'same', 'act_type': 'swish', 'norm_layer': None, 'norm_kwargs': {'eps': 0.001, 'momentum': 0.01}, 'box_class_repeats': 4, 'fpn_cell_repeats': 6, 'fpn_channels': 160, 'separable_conv': True, 'apply_resample_bn': True, 'conv_after_downsample': False, 'conv_bn_relu_pattern': False, 'use_native_resize_op': False, 'downsample_type': 'max', 'upsample_type': 'nearest', 'redundant_bias': True, 'head_bn_level_first': False, 'head_act_type': None, 'fpn_name': None, 'fpn_config': None, 'fpn_drop_path_rate': 0.0, 'alpha': 0.25, 'gamma': 1.5, 'label_smoothing': 0.0, 'legacy_focal': False, 'jit_loss': False, 'delta': 0.1, 'box_loss_weight': 50.0, 'soft_nms': False, 'max_detection_points': 5000, 'max_det_per_image': 100, 'url': 'https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/tf_efficientdet_d3_47-0b525f35.pth'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "dm = EfficientDetDataModule(\n",
    "        train_dataset_adaptor=train_ds, \n",
    "        validation_dataset_adaptor=val_ds,\n",
    "        train_transforms=get_train_transforms(train_image_size),\n",
    "        valid_transforms=get_valid_transforms(val_image_size),\n",
    "        num_workers=num_workers,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "\n",
    "model = EfficientDetModel(\n",
    "    num_classes=1,\n",
    "    img_size=train_image_size,\n",
    "    model_architecture=model_architecture,\n",
    "    prediction_confidence_threshold=prediction_confidence_threshold,\n",
    "    learning_rate=learning_rate,\n",
    "    wbf_iou_threshold=wbf_iou_threshold,\n",
    "    inference_transforms=get_valid_transforms(val_image_size),\n",
    "    )\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_mAP50',\n",
    "    dirpath=save_path,\n",
    "    filename=f'{exp_name}',\n",
    "    save_top_k=2,\n",
    "    mode='max',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "        gpus=[0], max_epochs=max_epochs, num_sanity_val_steps=1, callbacks=[checkpoint_callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a361f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name  | Type          | Params\n",
      "----------------------------------------\n",
      "0 | model | DetBenchTrain | 11.9 M\n",
      "----------------------------------------\n",
      "11.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.9 M    Total params\n",
      "47.613    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.03s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813a2209c328463997419d0b0938a8db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.25s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=4.77s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.66s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.020\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.002\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.024\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.25s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=4.61s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.60s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.001\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.022\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.027\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.25s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=4.70s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.57s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.003\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.005\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.006\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb11f4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2947529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e77255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1446cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7f8c76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413012bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8ab91e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5258f3e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13c040e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101a13fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d453fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
