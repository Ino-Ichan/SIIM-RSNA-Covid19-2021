{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9787587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "import cv2\n",
    "\n",
    "import albumentations\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, average_precision_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import os, sys, yaml\n",
    "\n",
    "sys.path.append('/workspace/siim-rsna-2021')\n",
    "from src.logger import setup_logger, LOGGER\n",
    "from src.meter import mAPMeter, AUCMeter, APMeter, AverageValueMeter\n",
    "from src.utils import plot_sample_images\n",
    "from src.segloss import SymmetricLovaszLoss\n",
    "\n",
    "\n",
    "# import neptune.new as neptune\n",
    "import wandb\n",
    "import pydicom\n",
    "\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import timm\n",
    "\n",
    "import warnings\n",
    "\n",
    "target_columns = [\n",
    "    \"Negative for Pneumonia\", \"Typical Appearance\", \"Indeterminate Appearance\", \"Atypical Appearance\", \"is_none\"\n",
    "]\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43279fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp.exp414.train import Net as Net414\n",
    "from exp.exp416.train import Net as Net416\n",
    "\n",
    "from exp.exp418.train import Net as Net418\n",
    "from exp.exp419.train import Net as Net419\n",
    "from exp.exp420.train import Net as Net420\n",
    "\n",
    "from exp.exp520.train import Net as Net520\n",
    "from exp.exp551.train import Net as Net551\n",
    "from exp.exp552.train import Net as Net552\n",
    "from exp.exp553.train import Net as Net553\n",
    "\n",
    "from exp.exp605.train import Net as Net605"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6081ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 df,\n",
    "                 image_size=512,\n",
    "                 transform=None,\n",
    "                 ):\n",
    "        self.df = df\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "        self.cols = target_columns\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        images = cv2.imread(row.npy_path)\n",
    "\n",
    "        # original image size\n",
    "        original_h = images.shape[0]\n",
    "        original_w = images.shape[1]\n",
    "        images = cv2.resize(images, (512, 512))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            aug = self.transform(image=images)\n",
    "            images_only = aug['image'].astype(np.float32).transpose(2, 0, 1) / 255\n",
    "        return {\n",
    "            \"image\": torch.tensor(images_only, dtype=torch.float),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6133ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_transforms(image_size=512):\n",
    "    return albumentations.Compose([\n",
    "        albumentations.Resize(image_size, image_size),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f61457a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_sub_list = [\n",
    "\n",
    "#     # prediction set, b6 mask\n",
    "#     [\n",
    "#         # backbone\n",
    "#         Net414(\"tf_efficientnetv2_m_in21k\"),\n",
    "#         \"tf_efficientnetv2_m_in21k\",\n",
    "#         # img_size\n",
    "#         512,\n",
    "#         # weight list\n",
    "#         [\n",
    "#             \"/workspace/output/exp414/model/cv0_weight_checkpoint_best.pth\",\n",
    "#             \"/workspace/output/exp414/model/cv1_weight_checkpoint_best.pth\",\n",
    "#             \"/workspace/output/exp414/model/cv2_weight_checkpoint_best.pth\",\n",
    "#             \"/workspace/output/exp414/model/cv3_weight_checkpoint_best.pth\",\n",
    "#             \"/workspace/output/exp414/model/cv4_weight_checkpoint_best.pth\",\n",
    "#         ],\n",
    "#     ],\n",
    "\n",
    "#     # prediction set, b7 mask\n",
    "#     [\n",
    "#         # backbone\n",
    "#         Net416(\"tf_efficientnetv2_m_in21k\"),\n",
    "#         \"tf_efficientnetv2_m_in21k\",\n",
    "#         # img_size\n",
    "#         512,\n",
    "#         # weight list\n",
    "#         [\n",
    "#             \"/workspace/output/exp416/model/cv0_weight_checkpoint_best.pth\",\n",
    "#             \"/workspace/output/exp416/model/cv1_weight_checkpoint_best.pth\",\n",
    "#             \"/workspace/output/exp416/model/cv2_weight_checkpoint_best.pth\",\n",
    "#             \"/workspace/output/exp416/model/cv3_weight_checkpoint_best.pth\",\n",
    "#             \"/workspace/output/exp416/model/cv4_weight_checkpoint_best.pth\",\n",
    "#         ],\n",
    "#     ],\n",
    "\n",
    "    # ===========================================\n",
    "    # Eff v2 L\n",
    "    # ===========================================\n",
    "\n",
    "\n",
    "    # prediction set, b7 map\n",
    "    [\n",
    "        # backbone\n",
    "        Net419(\"tf_efficientnetv2_l_in21k\"),\n",
    "        \"tf_efficientnetv2_l_in21k\",\n",
    "        # img_size\n",
    "        512,\n",
    "        # weight list\n",
    "        [\n",
    "            \"/workspace/output/exp419/model/cv0_weight_checkpoint_best.pth\",\n",
    "            \"/workspace/output/exp419/model/cv1_weight_checkpoint_best.pth\",\n",
    "            \"/workspace/output/exp419/model/cv2_weight_checkpoint_best.pth\",\n",
    "            \"/workspace/output/exp419/model/cv3_weight_checkpoint_best.pth\",\n",
    "            \"/workspace/output/exp419/model/cv4_weight_checkpoint_best.pth\",\n",
    "        ],\n",
    "    ],\n",
    "\n",
    "    # prediction set, b6, b7\n",
    "    [\n",
    "        # backbone\n",
    "        Net420(\"tf_efficientnetv2_l_in21k\"),\n",
    "        \"tf_efficientnetv2_l_in21k\",\n",
    "        # img_size\n",
    "        512,\n",
    "        # weight list\n",
    "        [\n",
    "            \"/workspace/output/exp420/model/cv0_weight_checkpoint_best.pth\",\n",
    "            \"/workspace/output/exp420/model/cv1_weight_checkpoint_best.pth\",\n",
    "            \"/workspace/output/exp420/model/cv2_weight_checkpoint_best.pth\",\n",
    "            \"/workspace/output/exp420/model/cv3_weight_checkpoint_best.pth\",\n",
    "            \"/workspace/output/exp420/model/cv4_weight_checkpoint_best.pth\",\n",
    "        ],\n",
    "    ],\n",
    "    \n",
    "    # prediction set, b6, b7\n",
    "    [\n",
    "        # backbone\n",
    "        Net420(\"tf_efficientnetv2_l_in21k\"),\n",
    "        \"tf_efficientnetv2_l_in21k\",\n",
    "        # img_size\n",
    "        512,\n",
    "        # weight list\n",
    "        [\n",
    "            \"/workspace/output/exp520/model/cv0_weight_checkpoint_best.pth\",\n",
    "            \"/workspace/output/exp520/model/cv1_weight_checkpoint_best.pth\",\n",
    "            \"/workspace/output/exp520/model/cv2_weight_checkpoint_best.pth\",\n",
    "            \"/workspace/output/exp520/model/cv3_weight_checkpoint_best.pth\",\n",
    "            \"/workspace/output/exp520/model/cv4_weight_checkpoint_best.pth\",\n",
    "        ],\n",
    "    ],\n",
    "    \n",
    "    # prediction set, b6\n",
    "    [\n",
    "        # backbone\n",
    "        Net418(\"tf_efficientnetv2_l_in21k\"),\n",
    "        \"tf_efficientnetv2_l_in21k\",\n",
    "        # img_size\n",
    "        512,\n",
    "        # weight list\n",
    "        [\n",
    "            \"/workspace/output/exp551/model/cv0_weight_checkpoint_best.pth\",\n",
    "            \"/workspace/output/exp551/model/cv1_weight_checkpoint_best.pth\",\n",
    "            \"/workspace/output/exp551/model/cv2_weight_checkpoint_best.pth\",\n",
    "            \"/workspace/output/exp551/model/cv3_weight_checkpoint_best.pth\",\n",
    "            \"/workspace/output/exp551/model/cv4_weight_checkpoint_best.pth\",\n",
    "        ],\n",
    "    ],\n",
    "\n",
    "    # ===========================================\n",
    "    # Swin transformer\n",
    "    # ===========================================\n",
    "\n",
    "#     # prediction set\n",
    "#     [\n",
    "#         # backbone\n",
    "#         Net605(\"swin_base_patch4_window12_384\"),\n",
    "#         \"swin_base_patch4_window12_384\",\n",
    "#         # img_size\n",
    "#         384,\n",
    "#         # weight list\n",
    "#         [\n",
    "#             \"/workspace/output/exp605/model/cv0_weight_checkpoint_best.pth\",\n",
    "#             \"/workspace/output/exp605/model/cv1_weight_checkpoint_best.pth\",\n",
    "#             \"/workspace/output/exp605/model/cv2_weight_checkpoint_best.pth\",\n",
    "#             \"/workspace/output/exp605/model/cv3_weight_checkpoint_best.pth\",\n",
    "#             \"/workspace/output/exp605/model/cv4_weight_checkpoint_best.pth\",\n",
    "#         ],\n",
    "#     ],\n",
    "    \n",
    "    \n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "970fd755",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d342681b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:02<00:08,  2.09s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:02<00:04,  1.60s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:03<00:02,  1.30s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:03<00:01,  1.05s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:04<00:00,  1.22it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:04<00:12,  4.10s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.70it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:01<00:01,  1.69it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:01<00:01,  1.79it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.86it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.89it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:06<00:07,  3.66s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.66it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:01<00:01,  1.65it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:01<00:01,  1.73it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.82it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.83it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:09<00:03,  3.39s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.71it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:01<00:01,  1.78it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:01<00:01,  1.71it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.80it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.85it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:12<00:00,  3.05s/it]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# key: image size, value: model\n",
    "model_dict = {}\n",
    "\n",
    "image_size_list = []\n",
    "\n",
    "for model_set in tqdm(study_sub_list):\n",
    "    # 画像サイズをkey, modelのリストをvalueにする\n",
    "    # keyがまだない場合はからのリストを登録\n",
    "    model_dict.setdefault(model_set[2], [])\n",
    "    model_list = []\n",
    "    for ckpt in tqdm(model_set[3]):\n",
    "        model = model_set[0].to(device)\n",
    "        weight = torch.load(ckpt, map_location=device)\n",
    "        model.load_state_dict(weight[\"state_dict\"])\n",
    "        model.eval()\n",
    "        model_list.append(deepcopy(model))\n",
    "    model_dict[model_set[2]] += model_list\n",
    "    image_size_list.append(model_set[2])\n",
    "    \n",
    "image_size_list = list(set(image_size_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fb36e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ext = pd.read_csv('/workspace/data/external/df_ext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bc93d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(df=df_ext, transform=get_val_transforms(512))\n",
    "test_loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=32,\n",
    "    num_workers=12,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b2114f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e25d92d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "553it [1:16:35,  8.31s/it]\n"
     ]
    }
   ],
   "source": [
    "pred_list1 = []\n",
    "pred_mask1 = []\n",
    "\n",
    "for i, image in tqdm(enumerate(test_loader)):\n",
    "#     if i == 3:\n",
    "#         break\n",
    "    pred_list2 = []\n",
    "    pred_mask2 = []\n",
    "#     image = dataset[i][\"image\"].to(device).unsqueeze(0)\n",
    "    image = image[\"image\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        for img_size, model_list in model_dict.items():\n",
    "            for model in model_list:\n",
    "                preds, *mask = model(image)\n",
    "                preds = preds.cpu().detach()\n",
    "                \n",
    "                pred_list2.append(preds.sigmoid())\n",
    "                pred_mask2 += [m.cpu().detach().sigmoid() for m in mask]\n",
    "                \n",
    "        # average prediction\n",
    "        pred_list1.append(torch.stack(pred_list2, 0).mean(0))\n",
    "        pred_mask1.append(torch.stack(pred_mask2, 0).mean(0))\n",
    "            \n",
    "preds = torch.cat(pred_list1).numpy()\n",
    "masks = torch.cat(pred_mask1).numpy()[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5304a0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17679, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "454ad074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17679, 16, 16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e5fec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/workspace/data/external/pseudo/preds_1', preds)\n",
    "np.save('/workspace/data/external/pseudo/masks_1', masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656e1d23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2443c39a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe0921f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b03f527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b327d4c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
